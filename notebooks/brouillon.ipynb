{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. GeoAI Integration\n",
        "\n",
        "Demonstrate how to integrate with GeoAI platforms and tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GeoAI Integration and Platform Connectivity\n",
        "# Next steps and recommendations\n",
        "print(f\"\\nüöÄ Recommended Next Steps:\")\n",
        "\n",
        "immediate_actions = [\n",
        "    \"Share results with emergency response teams\",\n",
        "    \"Validate findings with ground truth data\",\n",
        "    \"Coordinate with local authorities for field verification\",\n",
        "    \"Distribute maps to aid organizations\"\n",
        "]\n",
        "\n",
        "short_term_actions = [\n",
        "    \"Monitor recovery progress with follow-up imagery\",\n",
        "    \"Integrate results with emergency management systems\",\n",
        "    \"Develop real-time monitoring dashboard\",\n",
        "    \"Train local teams on the workflow\"\n",
        "]\n",
        "\n",
        "long_term_actions = [\n",
        "    \"Use results for reconstruction planning\",\n",
        "    \"Develop early warning systems\",\n",
        "    \"Create historical damage database\",\n",
        "    \"Establish operational monitoring protocols\"\n",
        "]\n",
        "\n",
        "print(\"\\n   üö® Immediate Actions (0-7 days):\")\n",
        "for action in immediate_actions:\n",
        "    print(f\"     ‚Ä¢ {action}\")\n",
        "\n",
        "print(\"\\n   üìÖ Short-term Actions (1-4 weeks):\")\n",
        "for action in short_term_actions:\n",
        "    print(f\"     ‚Ä¢ {action}\")\n",
        "\n",
        "print(\"\\n   üèóÔ∏è  Long-term Actions (1-6 months):\")\n",
        "for action in long_term_actions:\n",
        "    print(f\"     ‚Ä¢ {action}\")\n",
        "\n",
        "# Technical enhancement options\n",
        "print(f\"\\nüî¨ Technical Enhancement Options:\")\n",
        "\n",
        "enhancements = {\n",
        "    \"Data Quality\": [\n",
        "        \"Acquire higher resolution imagery (VHR satellites)\",\n",
        "        \"Integrate SAR coherence analysis\",\n",
        "        \"Add LiDAR data for 3D damage assessment\",\n",
        "        \"Include social media data for rapid assessment\"\n",
        "    ],\n",
        "    \"Analysis Methods\": [\n",
        "        \"Implement deep learning models (U-Net, DeepLabV3+)\",\n",
        "        \"Add building-level damage classification\",\n",
        "        \"Integrate infrastructure damage assessment\",\n",
        "        \"Develop multi-hazard analysis capabilities\"\n",
        "    ],\n",
        "    \"Automation\": [\n",
        "        \"Set up automated data acquisition pipelines\",\n",
        "        \"Implement real-time processing workflows\",\n",
        "        \"Create automated report generation\",\n",
        "        \"Develop API endpoints for system integration\"\n",
        "    ],\n",
        "    \"Validation\": [\n",
        "        \"Establish ground truth collection protocols\",\n",
        "        \"Implement accuracy assessment frameworks\",\n",
        "        \"Create uncertainty quantification methods\",\n",
        "        \"Develop cross-validation with multiple sensors\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, items in enhancements.items():\n",
        "    print(f\"\\n   üîß {category}:\")\n",
        "    for item in items:\n",
        "        print(f\"     ‚Ä¢ {item}\")\n",
        "\n",
        "# Integration opportunities\n",
        "print(f\"\\nüåê Integration Opportunities:\")\n",
        "\n",
        "integration_opportunities = {\n",
        "    \"Government Systems\": [\n",
        "        \"National disaster management platforms\",\n",
        "        \"Emergency response coordination centers\",\n",
        "        \"Municipal GIS systems\",\n",
        "        \"Infrastructure monitoring networks\"\n",
        "    ],\n",
        "    \"International Organizations\": [\n",
        "        \"UN-SPIDER knowledge portal\",\n",
        "        \"Copernicus Emergency Management Service\",\n",
        "        \"International Charter Space and Major Disasters\",\n",
        "        \"World Bank disaster risk management\"\n",
        "    ],\n",
        "    \"Academic Partnerships\": [\n",
        "        \"Research collaborations with universities\",\n",
        "        \"Student training and capacity building\",\n",
        "        \"Method validation and improvement\",\n",
        "        \"Publication of results and methods\"\n",
        "    ],\n",
        "    \"Private Sector\": [\n",
        "        \"Insurance company risk assessment\",\n",
        "        \"Engineering firm damage evaluation\",\n",
        "        \"Satellite data provider partnerships\",\n",
        "        \"Technology company collaborations\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, opportunities in integration_opportunities.items():\n",
        "    print(f\"\\n   ü§ù {category}:\")\n",
        "    for opportunity in opportunities:\n",
        "        print(f\"     ‚Ä¢ {opportunity}\")\n",
        "\n",
        "# Performance metrics and success indicators\n",
        "print(f\"\\nüìà Success Indicators:\")\n",
        "\n",
        "success_metrics = [\n",
        "    \"Response time: Assessment completed within 48-72 hours\",\n",
        "    \"Accuracy: >85% agreement with ground truth validation\",\n",
        "    \"Coverage: 100% of affected districts assessed\",\n",
        "    \"Accessibility: Results available in multiple formats\",\n",
        "    \"Actionability: Clear recommendations for response teams\",\n",
        "    \"Scalability: Workflow applicable to other disaster events\"\n",
        "]\n",
        "\n",
        "for i, metric in enumerate(success_metrics, 1):\n",
        "    print(f\"   {i}. {metric}\")\n",
        "\n",
        "# Contact and support information\n",
        "print(f\"\\nüìû Support and Resources:\")\n",
        "print(f\"\\n   üÜò Emergency Contact:\")\n",
        "print(f\"     ‚Ä¢ National Emergency Operations Center: +977-1-XXXXXXX\")\n",
        "print(f\"     ‚Ä¢ District Emergency Response: contact local authorities\")\n",
        "print(f\"     ‚Ä¢ International Aid Coordination: UN OCHA Nepal\")\n",
        "\n",
        "print(f\"\\n   üõ†Ô∏è  Technical Support:\")\n",
        "print(f\"     ‚Ä¢ Workflow documentation: README.md\")\n",
        "print(f\"     ‚Ä¢ Troubleshooting guide: docs/troubleshooting.md\")\n",
        "print(f\"     ‚Ä¢ Community forum: [project-forum-url]\")\n",
        "print(f\"     ‚Ä¢ GitHub issues: [project-github-url]/issues\")\n",
        "\n",
        "print(f\"\\n   üìö Additional Resources:\")\n",
        "print(f\"     ‚Ä¢ Training materials: docs/training/\")\n",
        "print(f\"     ‚Ä¢ Video tutorials: [training-video-url]\")\n",
        "print(f\"     ‚Ä¢ Best practices guide: docs/best-practices.md\")\n",
        "print(f\"     ‚Ä¢ API documentation: docs/api/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ü§ñ GeoAI Integration Workflow\n",
        "print(\"ü§ñ GeoAI Integration Workflow\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"\\nüåê Available GeoAI Platform Integrations:\")\n",
        "\n",
        "geoai_platforms = {\n",
        "    \"ESRI ArcGIS GeoAI\": {\n",
        "        \"description\": \"Pre-trained models from ArcGIS Living Atlas\",\n",
        "        \"capabilities\": [\n",
        "            \"Building footprint extraction\",\n",
        "            \"Land cover classification\",\n",
        "            \"Change detection models\",\n",
        "            \"Feature service integration\"\n",
        "        ],\n",
        "        \"integration\": \"ArcGIS Pro, Portal, Online\"\n",
        "    },\n",
        "    \"Google Earth Engine\": {\n",
        "        \"description\": \"Planetary-scale geospatial analysis\",\n",
        "        \"capabilities\": [\n",
        "            \"Massive satellite data archive\",\n",
        "            \"Cloud-based processing\",\n",
        "            \"Machine learning algorithms\",\n",
        "            \"Real-time monitoring\"\n",
        "        ],\n",
        "        \"integration\": \"Already integrated in data acquisition\"\n",
        "    },\n",
        "    \"Microsoft Planetary Computer\": {\n",
        "        \"description\": \"Open geospatial data and AI models\",\n",
        "        \"capabilities\": [\n",
        "            \"STAC-compliant data access\",\n",
        "            \"Azure ML integration\",\n",
        "            \"Scalable processing\",\n",
        "            \"Collaborative environment\"\n",
        "        ],\n",
        "        \"integration\": \"Azure notebooks, ML pipelines\"\n",
        "    },\n",
        "    \"NASA-IBM Prithvi\": {\n",
        "        \"description\": \"Foundation model for geospatial analysis\",\n",
        "        \"capabilities\": [\n",
        "            \"Pre-trained on satellite imagery\",\n",
        "            \"Fine-tuning for specific tasks\",\n",
        "            \"Multi-modal learning\",\n",
        "            \"Transfer learning\"\n",
        "        ],\n",
        "        \"integration\": \"Hugging Face, PyTorch\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for platform, info in geoai_platforms.items():\n",
        "    print(f\"\\nüîß {platform}:\")\n",
        "    print(f\"   {info['description']}\")\n",
        "    print(f\"   Integration: {info['integration']}\")\n",
        "    print(\"   Capabilities:\")\n",
        "    for cap in info['capabilities']:\n",
        "        print(f\"     - {cap}\")\n",
        "\n",
        "# Demonstrate integration preparation\n",
        "print(\"\\nüì§ Preparing data for GeoAI platforms...\")\n",
        "\n",
        "# Check available outputs for integration\n",
        "integration_ready = {}\n",
        "\n",
        "if 'reports_generated' in locals() and reports_generated:\n",
        "    if 'json' in reports_generated:\n",
        "        print(\"   ‚úì JSON format ready for API integration\")\n",
        "        integration_ready['api'] = reports_generated['json']\n",
        "    \n",
        "    if 'csv' in reports_generated:\n",
        "        print(\"   ‚úì CSV format ready for ML training\")\n",
        "        integration_ready['ml_training'] = reports_generated['csv']\n",
        "\n",
        "# Check for geospatial outputs\n",
        "if DISTRICTS_AVAILABLE:\n",
        "    # Save districts as various formats for different platforms\n",
        "    try:\n",
        "        # GeoJSON for web platforms\n",
        "        geojson_path = results_dir / 'affected_districts.geojson'\n",
        "        districts.to_file(geojson_path, driver='GeoJSON')\n",
        "        print(f\"   ‚úì GeoJSON saved: {geojson_path.name}\")\n",
        "        integration_ready['geojson'] = str(geojson_path)\n",
        "        \n",
        "        # KML for Google Earth\n",
        "        try:\n",
        "            kml_path = results_dir / 'affected_districts.kml'\n",
        "            districts.to_file(kml_path, driver='KML')\n",
        "            print(f\"   ‚úì KML saved: {kml_path.name}\")\n",
        "            integration_ready['kml'] = str(kml_path)\n",
        "        except Exception:\n",
        "            print(\"   ‚ö†Ô∏è  KML export requires additional dependencies\")\n",
        "        \n",
        "        # Shapefile for ESRI\n",
        "        try:\n",
        "            shapefile_path = results_dir / 'affected_districts.shp'\n",
        "            districts.to_file(shapefile_path, driver='ESRI Shapefile')\n",
        "            print(f\"   ‚úì Shapefile saved: {shapefile_path.name}\")\n",
        "            integration_ready['shapefile'] = str(shapefile_path)\n",
        "        except Exception:\n",
        "            print(\"   ‚ö†Ô∏è  Shapefile export failed\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Geospatial export failed: {e}\")\n",
        "\n",
        "print(\"\\nüîó Platform-Specific Integration Instructions:\")\n",
        "\n",
        "if 'shapefile' in integration_ready:\n",
        "    print(f\"\\nüìä ESRI ArcGIS Pro Integration:\")\n",
        "    print(f\"   1. Open ArcGIS Pro\")\n",
        "    print(f\"   2. Add data ‚Üí {Path(integration_ready['shapefile']).name}\")\n",
        "    print(f\"   3. Apply damage classification symbology\")\n",
        "    print(f\"   4. Use Spatial Analyst ‚Üí Hotspot Analysis\")\n",
        "    print(f\"   5. Create Web Map ‚Üí Share as Feature Service\")\n",
        "    print(f\"   6. Build StoryMap for public communication\")\n",
        "\n",
        "if 'geojson' in integration_ready:\n",
        "    print(f\"\\nüåç Web GIS Integration:\")\n",
        "    print(f\"   1. Upload {Path(integration_ready['geojson']).name} to:\")\n",
        "    print(f\"      - Mapbox Studio\")\n",
        "    print(f\"      - CARTO\")\n",
        "    print(f\"      - Leaflet/OpenLayers applications\")\n",
        "    print(f\"   2. Style with damage classification colors\")\n",
        "    print(f\"   3. Add interactive popups with statistics\")\n",
        "\n",
        "print(f\"\\n‚òÅÔ∏è  Cloud Platform Integration:\")\n",
        "print(f\"   Google Earth Engine Apps:\")\n",
        "print(f\"     - Upload results as Earth Engine assets\")\n",
        "print(f\"     - Create interactive web applications\")\n",
        "print(f\"     - Share with disaster response teams\")\n",
        "print(f\"\")\n",
        "print(f\"   Microsoft Azure:\")\n",
        "print(f\"     - Deploy to Azure Maps\")\n",
        "print(f\"     - Integrate with Azure ML pipelines\")\n",
        "print(f\"     - Use Power BI for dashboard creation\")\n",
        "print(f\"\")\n",
        "print(f\"   AWS:\")\n",
        "print(f\"     - Store in S3 with geospatial indexing\")\n",
        "print(f\"     - Process with SageMaker\")\n",
        "print(f\"     - Visualize with QuickSight\")"
      ]
    },
    {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "# Deep Learning Integration Example\n",
    "print(\"üß† Deep Learning Integration Example\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nüî¨ Preparing data for deep learning workflows...\")\n",
    "\n",
    "if ARD_AVAILABLE:\n",
    "    try:\n",
    "        # Example: Create PyTorch Dataset for damage detection\n",
    "        print(\"\\n‚ö° PyTorch Integration Example:\")\n",
    "        \n",
    "        pytorch_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.windows import Window\n",
    "\n",
    "class EarthquakeDamageDataset(Dataset):\n",
    "    \\\"\\\"\\\"\n",
    "    Custom dataset for earthquake damage detection\n",
    "    Combines pre and post-earthquake imagery with damage labels\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, pre_image_path, post_image_path, damage_path=None, \n",
    "                 patch_size=256, overlap=0.2, transform=None):\n",
    "        self.pre_path = pre_image_path\n",
    "        self.post_path = post_image_path\n",
    "        self.damage_path = damage_path\n",
    "        self.patch_size = patch_size\n",
    "        self.overlap = overlap\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Calculate patch grid\n",
    "        with rasterio.open(pre_image_path) as src:\n",
    "            self.height = src.height\n",
    "            self.width = src.width\n",
    "            self.crs = src.crs\n",
    "            self.transform_matrix = src.transform\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        step_size = int(patch_size * (1 - overlap))\n",
    "        self.patches_y = (self.height - patch_size) // step_size + 1\n",
    "        self.patches_x = (self.width - patch_size) // step_size + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.patches_y * self.patches_x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate patch coordinates\n",
    "        step_size = int(self.patch_size * (1 - self.overlap))\n",
    "        row = idx // self.patches_x\n",
    "        col = idx % self.patches_x\n",
    "        \n",
    "        y_start = row * step_size\n",
    "        x_start = col * step_size\n",
    "        \n",
    "        # Define window\n",
    "        window = Window(x_start, y_start, self.patch_size, self.patch_size)\n",
    "        \n",
    "        # Read patches\n",
    "        with rasterio.open(self.pre_path) as src:\n",
    "            pre_patch = src.read(window=window)\n",
    "        \n",
    "        with rasterio.open(self.post_path) as src:\n",
    "            post_patch = src.read(window=window)\n",
    "        \n",
    "        # Stack pre and post images (channels first)\n",
    "        combined = np.concatenate([pre_patch, post_patch], axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.FloatTensor(combined)\n",
    "        \n",
    "        # Handle labels if available\n",
    "        if self.damage_path:\n",
    "            with rasterio.open(self.damage_path) as src:\n",
    "                damage_patch = src.read(1, window=window)\n",
    "            label_tensor = torch.LongTensor(damage_patch)\n",
    "        else:\n",
    "            # Create dummy labels for inference\n",
    "            label_tensor = torch.zeros((self.patch_size, self.patch_size), dtype=torch.long)\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'label': label_tensor,\n",
    "            'coordinates': (y_start, x_start),\n",
    "            'patch_id': idx\n",
    "        }\n",
    "\n",
    "# Example CNN model for damage detection\n",
    "class DamageDetectionCNN(nn.Module):\n",
    "    def __init__(self, in_channels=8, num_classes=5):\n",
    "        super(DamageDetectionCNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Usage example:\n",
    "dataset = EarthquakeDamageDataset(\n",
    "    pre_image_path='pre_ard.tif',\n",
    "    post_image_path='post_ard.tif',\n",
    "    damage_path='damage_classification.tif'\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "model = DamageDetectionCNN(in_channels=8, num_classes=5)\n",
    "\n",
    "# Training loop would go here...\n",
    "'''\n",
    "        \n",
    "        print(\"Code template for PyTorch integration created:\")\n",
    "        print(\"  - Custom Dataset class for satellite imagery\")\n",
    "        print(\"  - CNN model for semantic segmentation\")\n",
    "        print(\"  - Patch-based processing for large images\")\n",
    "        print(\"  - Multi-temporal change detection support\")\n",
    "        \n",
    "        # Save PyTorch template\n",
    "        pytorch_template_path = results_dir / 'pytorch_integration_template.py'\n",
    "        with open(pytorch_template_path, 'w') as f:\n",
    "            f.write(pytorch_code)\n",
    "        \n",
    "        print(f\"\\n  ‚úì PyTorch template saved: {pytorch_template_path.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch template creation failed: {e}\")\n",
    "\n",
    "# TensorFlow/Keras integration example\n",
    "print(\"\\nüî• TensorFlow Integration Options:\")\n",
    "print(\"   - Use tf.data for efficient data loading\")\n",
    "print(\"   - Implement U-Net or DeepLabV3+ for segmentation\")\n",
    "print(\"   - Leverage TensorFlow Earth for geospatial ML\")\n",
    "print(\"   - Deploy with TensorFlow Serving\")\n",
    "\n",
    "# Hugging Face integration\n",
    "print(\"\\nü§ó Hugging Face Integration:\")\n",
    "print(\"   - Use Prithvi foundation model\")\n",
    "print(\"   - Fine-tune on earthquake damage data\")\n",
    "print(\"   - Share trained models on Model Hub\")\n",
    "print(\"   - Create Gradio demos\")\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "print(\"\\nüìä MLOps Integration:\")\n",
    "print(\"   MLflow for experiment tracking:\")\n",
    "print(\"     - Track model performance metrics\")\n",
    "print(\"     - Version control for datasets\")\n",
    "print(\"     - Model registry and deployment\")\n",
    "print(\"\")\n",
    "print(\"   Weights & Biases:\")\n",
    "print(\"     - Real-time monitoring\")\n",
    "print(\"     - Hyperparameter optimization\")\n",
    "print(\"     - Collaborative experiments\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps for Deep Learning:\")\n",
    "print(\"   1. Collect ground truth damage labels\")\n",
    "print(\"   2. Implement data augmentation\")\n",
    "print(\"   3. Train and validate models\")\n",
    "print(\"   4. Deploy for real-time inference\")\n",
    "print(\"   5. Create feedback loop for continuous learning\")"
  ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive workflow summary\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ EARTHQUAKE DAMAGE ASSESSMENT WORKFLOW COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Workflow status summary\n",
        "workflow_status = {\n",
        "    \"Setup & Configuration\": \"‚úÖ\",\n",
        "    \"Data Acquisition\": \"‚úÖ\" if CUSTOM_MODULES_AVAILABLE else \"üì¶ Demo\",\n",
        "    \"Image Preprocessing\": \"‚úÖ\" if ARD_AVAILABLE else \"‚ö†Ô∏è\",\n",
        "    \"Spectral Analysis\": \"‚úÖ\" if SPECTRAL_ANALYSIS_COMPLETE else \"‚ö†Ô∏è\",\n",
        "    \"Damage Assessment\": \"‚úÖ\" if COMPREHENSIVE_ASSESSMENT_COMPLETE else \"‚ö†Ô∏è\",\n",
        "    \"Visualization\": \"‚úÖ\",\n",
        "    \"Report Generation\": \"‚úÖ\" if reports_generated else \"‚ö†Ô∏è\",\n",
        "    \"GeoAI Integration\": \"‚úÖ\"\n",
        "}\n",
        "\n",
        "print(\"\\nüìä Workflow Status Summary:\")\n",
        "for step, status in workflow_status.items():\n",
        "    print(f\"   {status} {step}\")\n",
        "\n",
        "# Data directory summary\n",
        "print(f\"\\nüìÅ Output Directory Structure:\")\n",
        "print(f\"   üìÇ {data_dir}/\")\n",
        "\n",
        "subdirs = ['downloads', 'processed', 'results', 'ancillary', 'reports']\n",
        "for subdir in subdirs:\n",
        "    subdir_path = data_dir / subdir\n",
        "    if subdir_path.exists():\n",
        "        files = list(subdir_path.glob('*'))\n",
        "        print(f\"     üìÇ {subdir}/ ({len(files)} files)\")\n",
        "        # Show first few files\n",
        "        for file in files[:3]:\n",
        "            if file.is_file():\n",
        "                size_mb = file.stat().st_size / (1024 * 1024)\n",
        "                print(f\"       üìÑ {file.name} ({size_mb:.1f} MB)\")\n",
        "        if len(files) > 3:\n",
        "            print(f\"       ... and {len(files) - 3} more files\")\n",
        "    else:\n",
        "        print(f\"     üìÇ {subdir}/ (empty)\")\n",
        "\n",
        "# Key results summary\n",
        "print(f\"\\nüéØ Key Assessment Results:\")\n",
        "print(f\"   üìÖ Earthquake: {config['earthquake']['date']} - M{config['earthquake']['magnitude_ml']}\")\n",
        "print(f\"   üìç Location: {config['earthquake']['location']}\")\n",
        "print(f\"   üó∫Ô∏è  Affected Districts: {', '.join(config['earthquake']['affected_districts'])}\")\n",
        "\n",
        "if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals() and 'statistics' in results:\n",
        "    stats = results['statistics']\n",
        "    print(f\"   üìè Total Assessed Area: {stats.get('total_area_km2', 'N/A')} km¬≤\")\n",
        "    \n",
        "    if 'damage_distribution' in stats:\n",
        "        print(f\"\\n   üìä Damage Distribution:\")\n",
        "        for level, data in stats['damage_distribution'].items():\n",
        "            if data['area_km2'] > 0:\n",
        "                print(f\"     {level}: {data['area_km2']:.1f} km¬≤ ({data['percentage']:.1f}%)\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Detailed assessment results not available in demo mode\")\n",
        "\n",
        "# Next steps and recommendations\n",
        "print(\"\\nüöÄ Next Steps and Recommendations:\")\n",
        "print(\"   1. üö® Share results with emergency response teams immediately\")\n",
        "print(\"   2. üîç Validate findings with ground truth data collection\")\n",
        "print(\"   3. üì° Set up continuous monitoring for recovery tracking\")\n",
        "print(\"   4. ü§ù Coordinate with local authorities for field verification\")\n",
        "print(\"   5. üìä Integrate results into emergency management systems\")\n",
        "print(\"   6. üåç Share data with international disaster response organizations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create interactive map\n",
        "print(\"üåê Creating interactive damage map...\")\n",
        "\n",
        "try:\n",
        "    import folium\n",
        "    from folium import plugins\n",
        "    \n",
        "    # Create base map centered on epicenter\n",
        "    epicenter = config['earthquake']['epicenter']\n",
        "    m = folium.Map(\n",
        "        location=[epicenter[1], epicenter[0]],  # lat, lon\n",
        "        zoom_start=10,\n",
        "        tiles='OpenStreetMap'\n",
        "    )\n",
        "    \n",
        "    # Add epicenter marker\n",
        "    folium.Marker(\n",
        "        [epicenter[1], epicenter[0]],\n",
        "        popup=f\"<b>Epicenter</b><br>M{config['earthquake']['magnitude_ml']}<br>{config['earthquake']['date']}<br>{config['earthquake']['location']}\",\n",
        "        icon=folium.Icon(color='red', icon='star')\n",
        "    ).add_to(m)\n",
        "    \n",
        "    # Add affected districts\n",
        "    if DISTRICTS_AVAILABLE:\n",
        "        for idx, district in districts.iterrows():\n",
        "            # Add district boundaries\n",
        "            folium.GeoJson(\n",
        "                district.geometry,\n",
        "                style_function=lambda x: {\n",
        "                    'fillColor': 'lightblue',\n",
        "                    'color': 'blue',\n",
        "                    'weight': 2,\n",
        "                    'fillOpacity': 0.3\n",
        "                },\n",
        "                popup=f\"<b>{district['district']} District</b><br>Affected by earthquake\"\n",
        "            ).add_to(m)\n",
        "            \n",
        "            # Add district label\n",
        "            centroid = district.geometry.centroid\n",
        "            folium.Marker(\n",
        "                [centroid.y, centroid.x],\n",
        "                popup=f\"<b>{district['district']}</b>\",\n",
        "                icon=folium.DivIcon(\n",
        "                    html=f\"<div style='font-size: 12pt; font-weight: bold; color: darkblue;'>{district['district']}</div>\",\n",
        "                    icon_size=(80, 20),\n",
        "                    icon_anchor=(40, 10)\n",
        "                )\n",
        "            ).add_to(m)\n",
        "    \n",
        "    # Add damage data if available\n",
        "    if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals():\n",
        "        # Add damage statistics to map\n",
        "        if 'statistics' in results:\n",
        "            stats_html = \"<h4>Damage Assessment Summary</h4>\"\n",
        "            stats_html += f\"<b>Assessment Date:</b> {datetime.now().strftime('%Y-%m-%d')}<br>\"\n",
        "            stats_html += f\"<b>Total Area:</b> {results['statistics'].get('total_area_km2', 'N/A')} km¬≤<br><br>\"\n",
        "            \n",
        "            if 'damage_distribution' in results['statistics']:\n",
        "                stats_html += \"<b>Damage Distribution:</b><br>\"\n",
        "                for level, data in results['statistics']['damage_distribution'].items():\n",
        "                    if level != 'No Damage':\n",
        "                        stats_html += f\"{level}: {data['area_km2']:.1f} km¬≤ ({data['percentage']:.1f}%)<br>\"\n",
        "            \n",
        "            # Add statistics box\n",
        "            stats_box = folium.Html(stats_html, script=True)\n",
        "            popup = folium.Popup(stats_box, max_width=300)\n",
        "            \n",
        "            folium.Marker(\n",
        "                [epicenter[1] + 0.1, epicenter[0] + 0.1],\n",
        "                popup=popup,\n",
        "                icon=folium.Icon(color='green', icon='info-sign')\n",
        "            ).add_to(m)\n",
        "    \n",
        "    # Add additional map layers\n",
        "    folium.TileLayer('Esri.WorldImagery', name='Satellite').add_to(m)\n",
        "    folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m)\n",
        "    \n",
        "    # Add layer control\n",
        "    folium.LayerControl().add_to(m)\n",
        "    \n",
        "    # Add scale\n",
        "    plugins.MeasureControl().add_to(m)\n",
        "    \n",
        "    # Add minimap\n",
        "    minimap = plugins.MiniMap()\n",
        "    m.add_child(minimap)\n",
        "    \n",
        "    # Display map\n",
        "    display(m)\n",
        "    \n",
        "    # Save map\n",
        "    m.save(str(results_dir / 'interactive_damage_map.html'))\n",
        "    print(\"   ‚úì Interactive map saved to interactive_damage_map.html\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  Folium not available - interactive map not created\")\n",
        "    print(\"      Install with: pip install folium\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Interactive map creation failed: {e}\")"
      ]
    },
     {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot damage statistics\\n",
    "print(\"\\nCreating statistical visualizations...\")\n",
    "\\n",
    "if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals() and 'statistics' in results:\\n",
    "    try:\\n",
    "        stats = results['statistics']\\n",
    "        \\n",
    "        if 'damage_distribution' in stats:\\n",
    "            # Create comprehensive statistics plot\\n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\\n",
    "            \\n",
    "            # 1. Damage distribution pie chart\\n",
    "            damage_data = stats['damage_distribution']\\n",
    "            labels = list(damage_data.keys())\\n",
    "            sizes = [data['percentage'] for data in damage_data.values()]\\n",
    "            colors = ['#2E8B57', '#90EE90', '#FFD700', '#FF6347', '#8B0000']\\n",
    "            \\n",
    "            # Only show non-zero categories\\n",
    "            non_zero_mask = np.array(sizes) > 0\\n",
    "            labels_filtered = [labels[i] for i in range(len(labels)) if non_zero_mask[i]]\\n",
    "            sizes_filtered = [sizes[i] for i in range(len(sizes)) if non_zero_mask[i]]\\n",
    "            colors_filtered = [colors[i] for i in range(len(colors)) if non_zero_mask[i]]\\n",
    "            \\n",
    "            wedges, texts, autotexts = ax1.pie(sizes_filtered, labels=labels_filtered, \\n",
    "                                             colors=colors_filtered, autopct='%1.1f%%',\\n",
    "                                             startangle=90, textprops={'fontsize': 10})\\n",
    "            ax1.set_title('Damage Distribution by Area', fontsize=12, fontweight='bold')\\n",
    "            \\n",
    "            # 2. Damage distribution bar chart\\n",
    "            areas = [data['area_km2'] for data in damage_data.values()]\\n",
    "            bars = ax2.bar(labels, areas, color=colors[:len(labels)])\\n",
    "            ax2.set_title('Damage Area by Level (km¬≤)', fontsize=12, fontweight='bold')\\n",
    "            ax2.set_ylabel('Area (km¬≤)')\\n",
    "            ax2.tick_params(axis='x', rotation=45)\\n",
    "            \\n",
    "            # Add value labels on bars\\n",
    "            for bar, area in zip(bars, areas):\\n",
    "                height = bar.get_height()\\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height,\\n",
    "                        f'{area:.1f}', ha='center', va='bottom', fontsize=9)\\n",
    "            \\n",
    "            # 3. Severity assessment\\n",
    "            # Calculate severity score\\n",
    "            severity_weights = {'No Damage': 0, 'Low': 1, 'Moderate': 2, 'High': 3, 'Severe': 4}\\n",
    "            weighted_score = sum(data['percentage'] * severity_weights.get(level, 0) \\n",
    "                                for level, data in damage_data.items()) / 100\\n",
    "            \\n",
    "            # Severity gauge\\n",
    "            theta = np.linspace(0, np.pi, 100)\\n",
    "            r = np.ones_like(theta)\\n",
    "            \\n",
    "            # Color zones\\n",
    "            ax3.fill_between(theta[0:20], 0, r[0:20], color='green', alpha=0.3, label='Low')\\n",
    "            ax3.fill_between(theta[20:40], 0, r[20:40], color='yellow', alpha=0.3, label='Moderate')\\n",
    "            ax3.fill_between(theta[40:60], 0, r[40:60], color='orange', alpha=0.3, label='High')\\n",
    "            ax3.fill_between(theta[60:80], 0, r[60:80], color='red', alpha=0.3, label='Severe')\\n",
    "            ax3.fill_between(theta[80:100], 0, r[80:100], color='darkred', alpha=0.3, label='Critical')\\n",
    "            \\n",
    "            # Severity needle\\n",
    "            needle_angle = np.pi * (1 - weighted_score / 4)\\n",
    "            ax3.arrow(0, 0, 0.8 * np.cos(needle_angle), 0.8 * np.sin(needle_angle),\\n",
    "                     head_width=0.05, head_length=0.1, fc='black', ec='black', linewidth=3)\\n",
    "            \\n",
    "            ax3.set_xlim(-1.2, 1.2)\\n",
    "            ax3.set_ylim(0, 1.2)\\n",
    "            ax3.set_aspect('equal')\\n",
    "            ax3.axis('off')\\n",
    "            ax3.set_title(f'Overall Severity Score: {weighted_score:.2f}/4.0', \\n",
    "                         fontsize=12, fontweight='bold')\\n",
    "            \\n",
    "            # 4. Assessment timeline\\n",
    "            # Create timeline of key dates\\n",
    "            from datetime import datetime, timedelta\\n",
    "            import matplotlib.dates as mdates\\n",
    "            \\n",
    "            earthquake_date = datetime.strptime(config['earthquake']['date'], '%Y-%m-%d')\\n",
    "            assessment_date = datetime.now()\\n",
    "            \\n",
    "            timeline_events = [\\n",
    "                (earthquake_date - timedelta(days=30), 'Pre-event baseline', 'blue'),\\n",
    "                (earthquake_date, 'Earthquake (M6.4)', 'red'),\\n",
    "                (earthquake_date + timedelta(days=1), 'Post-event monitoring starts', 'orange'),\\n",
    "                (assessment_date, 'Damage assessment', 'green')\\n",
    "            ]\\n",
    "            \\n",
    "            dates, events, colors_timeline = zip(*timeline_events)\\n",
    "            \\n",
    "            ax4.scatter(dates, range(len(dates)), c=colors_timeline, s=100, zorder=3)\\n",
    "            \\n",
    "            for i, (date, event, color) in enumerate(timeline_events):\\n",
    "                ax4.annotate(event, (date, i), xytext=(10, 0), \\n",
    "                           textcoords='offset points', va='center', fontsize=9)\\n",
    "            \\n",
    "            ax4.set_ylim(-0.5, len(dates) - 0.5)\\n",
    "            ax4.set_xlabel('Date')\\n",
    "            ax4.set_title('Assessment Timeline', fontsize=12, fontweight='bold')\\n",
    "            ax4.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\\n",
    "            ax4.grid(True, alpha=0.3)\\n",
    "            \\n",
    "            plt.tight_layout()\\n",
    "            plt.suptitle(f'Comprehensive Damage Assessment Statistics\\\\n{config[\\\"earthquake\\\"][\\\"location\\\"]} - {config[\\\"earthquake\\\"][\\\"date\\\"]}',\\n",
    "                        fontsize=14, fontweight='bold', y=0.98)\\n",
    "            plt.subplots_adjust(top=0.93)\\n",
    "            plt.show()\\n",
    "            \\n",
    "            # Save statistics plot\\n",
    "            fig.savefig(results_dir / 'damage_statistics.png', \\n",
    "                       dpi=300, bbox_inches='tight')\\n",
    "            print(\"\\   ‚úì Statistical visualizations saved\")\n",
    "            \\n",
    "    except Exception as e:\\n",
    "        print(f\"\\  ‚ùå Statistical visualization failed: {e}\")\\n",
    "        traceback.print_exc()\\n",
    "else:\\n",
    "    print(\"\\   ‚ö†Ô∏è  No statistical data available for visualization\"\\)"
   ]
   
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Report Generation\\n",
    "\\n",
    "Generate comprehensive reports in multiple formats."
   ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Report generation workflow\n",
        "print(\"üìÑ Report Generation Workflow\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "reports_generated = {}\n",
        "\n",
        "if CUSTOM_MODULES_AVAILABLE:\n",
        "    try:\n",
        "        # Initialize report generator\n",
        "        print(\"üîß Initializing report generator...\")\n",
        "        reporter = ReportGenerator('config.json')\n",
        "        \n",
        "        print(\"\\nüìã Generating comprehensive reports...\")\n",
        "        print(\"  Report types:\")\n",
        "        print(\"   1. üìÑ PDF executive summary\")\n",
        "        print(\"   2. üìä Excel workbook with detailed data\")\n",
        "        print(\"   3. üóÇÔ∏è  GIS outputs (Shapefile, GeoPackage, KML)\")\n",
        "        print(\"   4. üåê Web-based interactive report\")\n",
        "        print(\"   5. üìà Technical analysis document\")\n",
        "        \n",
        "        # Generate all reports\n",
        "        reports = reporter.generate_all_reports()\n",
        "        reports_generated = reports\n",
        "        \n",
        "        print(\"\\n‚úÖ Reports Generation Complete!\")\n",
        "        print(\"\\nGenerated reports:\")\n",
        "        for report_type, path in reports.items():\n",
        "            if isinstance(path, dict):\n",
        "                print(f\"\\n{report_type.upper()}:\")\n",
        "                for sub_type, sub_path in path.items():\n",
        "                    print(f\"   - {sub_type}: {sub_path}\")\n",
        "            else:\n",
        "                print(f\"   {report_type}: {path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Report generation failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"üì¶ Demo Mode - Creating sample reports...\")\n",
        "    \n",
        "    try:\n",
        "        # Create sample HTML report\n",
        "        print(\"\\nüåê Creating sample HTML report...\")\n",
        "        \n",
        "        html_content = f'''\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Nepal Earthquake Damage Assessment Report</title>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "                .header {{ background-color: #2c3e50; color: white; padding: 20px; text-align: center; }}\n",
        "                .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #3498db; }}\n",
        "                .stats {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; }}\n",
        "                .warning {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; }}\n",
        "                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}\n",
        "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "                th {{ background-color: #f2f2f2; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"header\">\n",
        "                <h1>Earthquake Damage Assessment Report</h1>\n",
        "                <h2>{config['earthquake']['location']}</h2>\n",
        "                <p>Assessment Date: {datetime.now().strftime('%B %d, %Y')}</p>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"section\">\n",
        "                <h2>üåç Event Summary</h2>\n",
        "                <div class=\"stats\">\n",
        "                    <p><strong>Date:</strong> {config['earthquake']['date']} {config['earthquake']['time']}</p>\n",
        "                    <p><strong>Magnitude:</strong> {config['earthquake']['magnitude_ml']} ML ({config['earthquake']['magnitude_mw']} Mw)</p>\n",
        "                    <p><strong>Depth:</strong> {config['earthquake']['depth_km']} km</p>\n",
        "                    <p><strong>Epicenter:</strong> {config['earthquake']['epicenter'][1]}¬∞N, {config['earthquake']['epicenter'][0]}¬∞E</p>\n",
        "                    <p><strong>Affected Districts:</strong> {', '.join(config['earthquake']['affected_districts'])}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        '''\n",
        "        \n",
        "        # Add damage assessment results if available\n",
        "        if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals():\n",
        "            html_content += f'''\n",
        "            <div class=\"section\">\n",
        "                <h2>üìä Damage Assessment Results</h2>\n",
        "                <div class=\"stats\">\n",
        "                    <p><strong>Total Assessed Area:</strong> {results['statistics'].get('total_area_km2', 'N/A')} km¬≤</p>\n",
        "                    <p><strong>Analysis Method:</strong> Multi-spectral satellite imagery analysis</p>\n",
        "                    <p><strong>Data Sources:</strong> Sentinel-2, Landsat, SRTM DEM</p>\n",
        "                </div>\n",
        "                \n",
        "                <h3>Damage Distribution</h3>\n",
        "                <table>\n",
        "                    <tr><th>Damage Level</th><th>Area (km¬≤)</th><th>Percentage</th></tr>\n",
        "            '''\n",
        "            \n",
        "            if 'damage_distribution' in results['statistics']:\n",
        "                for level, data in results['statistics']['damage_distribution'].items():\n",
        "                    html_content += f'''\n",
        "                    <tr>\n",
        "                        <td>{level}</td>\n",
        "                        <td>{data['area_km2']:.2f}</td>\n",
        "                        <td>{data['percentage']:.1f}%</td>\n",
        "                    </tr>\n",
        "                    '''\n",
        "            \n",
        "            html_content += \"</table></div>\"\n",
        "        \n",
        "        # Add methodology and conclusions\n",
        "        html_content += f'''\n",
        "            <div class=\"section\">\n",
        "                <h2>üî¨ Methodology</h2>\n",
        "                <p>This assessment utilized satellite-based remote sensing techniques including:</p>\n",
        "                <ul>\n",
        "                    <li>Multi-temporal change detection using pre and post-earthquake imagery</li>\n",
        "                    <li>Spectral index analysis (NDVI, NBR, NDBI, BSI)</li>\n",
        "                    <li>Machine learning classification algorithms</li>\n",
        "                    <li>Integration with auxiliary datasets (DEM, population, infrastructure)</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"section\">\n",
        "                <h2>‚ö†Ô∏è Important Notes</h2>\n",
        "                <div class=\"warning\">\n",
        "                    <p><strong>Validation Required:</strong> This automated assessment should be validated with ground truth data and field surveys.</p>\n",
        "                    <p><strong>Weather Conditions:</strong> Cloud cover and atmospheric conditions may affect accuracy.</p>\n",
        "                    <p><strong>Resolution Limitations:</strong> Satellite imagery resolution may not capture all building-level damage.</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"section\">\n",
        "                <h2>üìû Contact Information</h2>\n",
        "                <p>For questions about this assessment, please contact:</p>\n",
        "                <p>Emergency Response Team<br>\n",
        "                Email: response@emergency.gov.np<br>\n",
        "                Phone: +977-1-XXXXXXX</p>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"section\">\n",
        "                <p><em>Report generated automatically using GeoAI damage assessment workflow</em></p>\n",
        "                <p><em>Generation time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</em></p>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        '''\n",
        "        \n",
        "        # Save HTML report\n",
        "        html_report_path = results_dir / 'damage_assessment_report.html'\n",
        "        with open(html_report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(html_content)\n",
        "        \n",
        "        print(f\"   ‚úì HTML report saved: {html_report_path}\")\n",
        "        reports_generated['html'] = str(html_report_path)\n",
        "        \n",
        "        # Create summary JSON report\n",
        "        print(\"\\nüìä Creating JSON summary report...\")\n",
        "        \n",
        "        summary_report = {\n",
        "            \"metadata\": {\n",
        "                \"report_type\": \"earthquake_damage_assessment\",\n",
        "                \"version\": \"1.0\",\n",
        "                \"generated_at\": datetime.now().isoformat(),\n",
        "                \"assessment_area\": config['earthquake']['location'],\n",
        "                \"earthquake_date\": config['earthquake']['date']\n",
        "            },\n",
        "            \"earthquake_info\": config['earthquake'],\n",
        "            \"analysis_parameters\": config['analysis_parameters'],\n",
        "            \"assessment_results\": {}\n",
        "        }\n",
        "        \n",
        "        if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals():\n",
        "            summary_report[\"assessment_results\"] = results.get('statistics', {})\n",
        "        \n",
        "        # Save JSON report\n",
        "        json_report_path = results_dir / 'assessment_summary.json'\n",
        "        with open(json_report_path, 'w') as f:\n",
        "            json.dump(summary_report, f, indent=4)\n",
        "        \n",
        "        print(f\"   ‚úì JSON summary saved: {json_report_path}\")\n",
        "        reports_generated['json'] = str(json_report_path)\n",
        "        \n",
        "        # Create CSV data export\n",
        "        if COMPREHENSIVE_ASSESSMENT_COMPLETE and 'results' in locals():\n",
        "            print(\"\\nüìà Creating CSV data export...\")\n",
        "            \n",
        "            # Create damage statistics CSV\n",
        "            damage_stats = []\n",
        "            if 'damage_distribution' in results['statistics']:\n",
        "                for level, data in results['statistics']['damage_distribution'].items():\n",
        "                    damage_stats.append({\n",
        "                        'damage_level': level,\n",
        "                        'area_km2': data['area_km2'],\n",
        "                        'percentage': data['percentage']\n",
        "                    })\n",
        "            \n",
        "            if damage_stats:\n",
        "                csv_path = results_dir / 'damage_statistics.csv'\n",
        "                pd.DataFrame(damage_stats).to_csv(csv_path, index=False)\n",
        "                print(f\"   ‚úì CSV export saved: {csv_path}\")\n",
        "                reports_generated['csv'] = str(csv_path)\n",
        "        \n",
        "        print(\"\\n‚úÖ Demo reports created successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Demo report creation failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Display report summary\n",
        "print(f\"\\nüìã Report Generation Summary:\")\n",
        "if reports_generated:\n",
        "    for report_type, path in reports_generated.items():\n",
        "        print(f\"   ‚úì {report_type.upper()}: {Path(path).name}\")\n",
        "    print(f\"\\nüìÅ All reports saved to: {results_dir}\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No reports generated\")\n",
        "\n",
        "print(f\"\\nüí° Reports can be shared with:\")\n",
        "print(\"   - Emergency response teams\")\n",
        "print(\"   - Government agencies\")\n",
        "print(\"   - International aid organizations\")\n",
        "print(\"   - Scientific community\")\n",
        "print(\"   - Insurance companies\")\n",
        "print(\"   - Reconstruction planning committees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization\n",
        "\n",
        "Create various visualizations of the damage assessment results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Damage visualization workflow\n",
        "print(\"üé® Damage Visualization Workflow\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if CUSTOM_MODULES_AVAILABLE:\n",
        "    try:\n",
        "        # Initialize visualizer\n",
        "        print(\"üîß Initializing damage visualizer...\")\n",
        "        visualizer = DamageVisualizer('config.json')\n",
        "\n",
        "        # Create before/after comparison\n",
        "        if ARD_AVAILABLE and pre_ard.exists() and post_ard.exists():\n",
        "            print(\"\\nüñºÔ∏è  Creating before/after comparison...\")\n",
        "            fig = visualizer.create_before_after_comparison(\n",
        "                str(pre_ard),\n",
        "                str(post_ard)\n",
        "            )\n",
        "            plt.show()\n",
        "            print(\"   ‚úì Before/after comparison created\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Visualizer initialization failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"üì¶ Demo Mode - Creating visualizations...\")\n",
        "\n",
        "    # Create before/after comparison visualization\n",
        "    if ARD_AVAILABLE:\n",
        "        try:\n",
        "            print(\"\\nüñºÔ∏è  Creating before/after comparison...\")\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "            # Load and display ARD files\n",
        "            try:\n",
        "                import rasterio\n",
        "\n",
        "                with rasterio.open(pre_ard) as src:\n",
        "                    pre_data = src.read([3, 2, 1])  # RGB bands\n",
        "                    pre_data = np.transpose(pre_data, (1, 2, 0))\n",
        "                    # Normalize for display\n",
        "                    pre_data = np.clip(pre_data / np.percentile(pre_data, 98), 0, 1)\n",
        "                with rasterio.open(post_ard) as src:\n",
        "                    post_data = src.read([3, 2, 1])  # RGB bands\n",
        "                    post_data = np.transpose(post_data, (1, 2, 0))\n",
        "                    # Normalize for display\n",
        "                    post_data = np.clip(post_data / np.percentile(post_data, 98), 0, 1)\n",
        "\n",
        "                axes[0].imshow(pre_data)\n",
        "                axes[0].set_title(\n",
        "                    'Pre-Earthquake\\n' + config['date_ranges']['pre_end'],\n",
        "                    fontsize=14, fontweight='bold'\n",
        "                )\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                axes[1].imshow(post_data)\n",
        "                axes[1].set_title(\n",
        "                    'Post-Earthquake\\n' + config['date_ranges']['post_start'],\n",
        "                    fontsize=14, fontweight='bold'\n",
        "                )\n",
        "                axes[1].axis('off')\n",
        "\n",
        "            except ImportError:\n",
        "                # Fallback without rasterio\n",
        "                axes[0].text(\n",
        "                    0.5, 0.5,\n",
        "                    'Pre-Earthquake\\nImage',\n",
        "                    ha='center', va='center',\n",
        "                    transform=axes[0].transAxes,\n",
        "                    fontsize=16,\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightblue')\n",
        "                )\n",
        "                axes[0].set_title('Pre-Earthquake', fontsize=14, fontweight='bold')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                axes[1].text(\n",
        "                    0.5, 0.5,\n",
        "                    'Post-Earthquake\\nImage',\n",
        "                    ha='center', va='center',\n",
        "                    transform=axes[1].transAxes,\n",
        "                    fontsize=16,\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightcoral')\n",
        "                )\n",
        "                axes[1].set_title('Post-Earthquake', fontsize=14, fontweight='bold')\n",
        "                axes[1].axis('off')\n",
        "\n",
        "            plt.suptitle(\n",
        "                f\"Satellite Imagery Comparison\\n{config['earthquake']['location']} - {config['earthquake']['date']}\",\n",
        "                fontsize=16, fontweight='bold', y=0.95\n",
        "            )\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Save comparison\n",
        "            fig.savefig(\n",
        "                results_dir / 'before_after_comparison.png',\n",
        "                dpi=300, bbox_inches='tight'\n",
        "            )\n",
        "            print(\"   ‚úì Before/after comparison saved\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not create before/after comparison: {e}\")\n",
        "\n",
        "# Visualize damage map\n",
        "print(\"\\nüó∫Ô∏è  Creating damage classification map...\")\n",
        "if COMPREHENSIVE_ASSESSMENT_COMPLETE:\n",
        "    try:\n",
        "        damage_map_data = None\n",
        "        \n",
        "        if CUSTOM_MODULES_AVAILABLE:\n",
        "            # Try to load from analyzer results\n",
        "            damage_map_path = results_dir / 'damage_classification.tif'\n",
        "            if damage_map_path.exists():\n",
        "                try:\n",
        "                    import rasterio\n",
        "                    with rasterio.open(damage_map_path) as src:\n",
        "                        damage_map_data = src.read(1)\n",
        "                except ImportError:\n",
        "                    pass\n",
        "        \n",
        "        # Use synthetic data if available\n",
        "        if damage_map_data is None and 'results' in locals() and 'damage_classification' in results:\n",
        "            damage_map_data = results['damage_classification']\n",
        "        \n",
        "        if damage_map_data is not None:\n",
        "            fig, ax = plt.subplots(figsize=(12, 10))\n",
        "            \n",
        "            # Define damage level colors and labels\n",
        "            damage_colors = ['#2E8B57', '#90EE90', '#FFD700', '#FF6347', '#8B0000']\n",
        "            damage_labels = ['No Damage', 'Low', 'Moderate', 'High', 'Severe']\n",
        "            \n",
        "            # Create custom colormap\n",
        "            from matplotlib.colors import ListedColormap\n",
        "            damage_cmap = ListedColormap(damage_colors)\n",
        "            \n",
        "            # Display damage map\n",
        "            im = ax.imshow(damage_map_data, cmap=damage_cmap, vmin=0, vmax=4)\n",
        "            \n",
        "            # Add colorbar with labels\n",
        "            cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)\n",
        "            cbar.set_ticks(range(5))\n",
        "            cbar.set_ticklabels(damage_labels)\n",
        "            cbar.set_label('Damage Level', fontsize=12, fontweight='bold')\n",
        "            \n",
        "            ax.set_title(\n",
        "                f\"Earthquake Damage Assessment Map\\n{config['earthquake']['location']} - {config['earthquake']['date']}\",\n",
        "                fontsize=14, fontweight='bold', pad=20\n",
        "            )\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Add statistics text\n",
        "            if 'results' in locals() and 'statistics' in results:\n",
        "                stats_text = \"Damage Summary:\\n\"\n",
        "                for level, data in results['statistics']['damage_distribution'].items():\n",
        "                    if level != 'No Damage':\n",
        "                        stats_text += f\"{level}: {data['area_km2']:.1f} km¬≤\\n\"\n",
        "                ax.text(\n",
        "                    0.02, 0.98, stats_text,\n",
        "                    transform=ax.transAxes,\n",
        "                    fontsize=10, verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9)\n",
        "                )\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # Save damage map\n",
        "            fig.savefig(\n",
        "                results_dir / 'damage_classification_map.png',\n",
        "                dpi=300, bbox_inches='tight'\n",
        "            )\n",
        "            print(\"   ‚úì Damage classification map saved\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  No damage classification data available\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Damage map visualization failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Comprehensive assessment not complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nepal Earthquake Damage Assessment Workflow\n",
        "\n",
        "This notebook provides a complete workflow for post-disaster damage assessment using satellite imagery and GeoAI techniques.\n",
        "\n",
        "## Event Information\n",
        "- **Date**: November 3, 2023\n",
        "- **Magnitude**: 6.4 ML (5.7 Mw)\n",
        "- **Epicenter**: Ramidanda, Jajarkot District (28.84¬∞N, 82.19¬∞E)\n",
        "- **Affected Districts**: Jajarkot, Rukum West, Salyan\n",
        "\n",
        "## Prerequisites\n",
        "Before running this notebook, ensure you have:\n",
        "1. Required Python packages installed (see requirements.txt)\n",
        "2. Google Earth Engine account and authentication\n",
        "3. API keys for satellite data providers (optional)\n",
        "4. Custom modules in the scripts/ directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML, Image\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set matplotlib backend for better compatibility\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Set up paths with error handling\n",
        "try:\n",
        "    SCRIPT_DIR = Path('./scripts')\n",
        "    SCRIPT_DIR.mkdir(exist_ok=True)  # Create if doesn't exist\n",
        "    sys.path.append(str(SCRIPT_DIR))\n",
        "\n",
        "    # Import custom modules with error handling\n",
        "    try:\n",
        "        from scripts.data_acquisition import EarthquakeDataAcquisition\n",
        "        from scripts.preprocessing import ImagePreprocessor\n",
        "        from scripts.damage_analysis import DamageAnalyzer\n",
        "        from scripts.visualization import DamageVisualizer\n",
        "        from scripts.reporting import ReportGenerator\n",
        "        CUSTOM_MODULES_AVAILABLE = True\n",
        "        print(\"‚úì Custom modules imported successfully\")\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  Custom modules not found: {e}\")\n",
        "        print(\"   Notebook will run in demo mode with limited functionality\")\n",
        "        CUSTOM_MODULES_AVAILABLE = False\n",
        "\n",
        "    print(\"‚úì Setup complete!\")\n",
        "    print(f\"   Working directory: {Path.cwd()}\")\n",
        "    print(f\"   Scripts directory: {SCRIPT_DIR}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup failed: {e}\")\n",
        "    print(\"   Please check your environment and file structure\")\n"
      ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Create or load the configuration file with earthquake parameters and data paths."
   ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with enhanced structure\n",
    "config = {\n",
    "    \"project\": {\n",
    "        \"name\": \"Nepal_Earthquake_2023\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"created\": datetime.now().isoformat(),\n",
    "        \"description\": \"Damage assessment for November 3, 2023 earthquake in Nepal\"\n",
    "    },\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"earthquake\": {\n",
    "        \"date\": \"2023-11-03\",\n",
    "        \"time\": \"23:47:14 UTC\",\n",
    "        \"epicenter\": [82.19, 28.84],  # [longitude, latitude]\n",
    "        \"magnitude_ml\": 6.4,\n",
    "        \"magnitude_mw\": 5.7,\n",
    "        \"depth_km\": 18,\n",
    "        \"location\": \"Ramidanda, Jajarkot District\",\n",
    "        \"affected_districts\": [\"Jajarkot\", \"Rukum West\", \"Salyan\"],\n",
    "        \"country\": \"Nepal\"\n",
    "    },\n",
    "    \"date_ranges\": {\n",
    "        \"pre_start\": \"2023-09-01\",\n",
    "        \"pre_end\": \"2023-11-02\",\n",
    "        \"post_start\": \"2023-11-04\",\n",
    "        \"post_end\": \"2023-12-15\"\n",
    "    },\n",
    "    \"analysis_parameters\": {\n",
    "        \"aoi_buffer_km\": 50,\n",
    "        \"cloud_threshold\": 20,\n",
    "        \"resolution_m\": 10,\n",
    "        \"patch_size\": 256,\n",
    "        \"overlap_ratio\": 0.2\n",
    "    },\n",
    "    \"data_sources\": {\n",
    "        \"optical\": [\"Sentinel-2\", \"Landsat-8\", \"Landsat-9\"],\n",
    "        \"sar\": [\"Sentinel-1\"],\n",
    "        \"dem\": \"SRTM\",\n",
    "        \"osm\": True,\n",
    "        \"population\": \"WorldPop\"\n",
    "    },\n",
    "    \"apis\": {\n",
    "        \"planet_api_key\": \"\",  # Add your API key if available\n",
    "        \"maxar_api_key\": \"\",   # Add your API key if available\n",
    "        \"copernicus_user\": \"\",\n",
    "        \"copernicus_pass\": \"\",\n",
    "        \"gee_service_account\": \"\"  # Path to service account JSON\n",
    "    },\n",
    "    \"output_formats\": {\n",
    "        \"raster\": [\"GeoTIFF\", \"COG\"],\n",
    "        \"vector\": [\"GeoJSON\", \"Shapefile\", \"GeoPackage\"],\n",
    "        \"reports\": [\"PDF\", \"HTML\", \"Excel\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create data directory structure\n",
    "try:\n",
    "    data_dir = Path(config['data_dir'])\n",
    "    subdirs = ['downloads', 'processed', 'results', 'ancillary', 'reports']\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        (data_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = 'config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(\"‚úì Configuration saved and directories created!\")\n",
    "    print(f\"   Config file: {config_path}\")\n",
    "    print(f\"   Data directory: {data_dir.absolute()}\")\n",
    "    print(f\"\\nüìç Earthquake Details:\")\n",
    "    print(f\"   Date: {config['earthquake']['date']} {config['earthquake']['time']}\")\n",
    "    print(f\"   Magnitude: {config['earthquake']['magnitude_ml']} ML ({config['earthquake']['magnitude_mw']} Mw)\")\n",
    "    print(f\"   Location: {config['earthquake']['location']}\")\n",
    "    print(f\"   Epicenter: {config['earthquake']['epicenter'][1]}¬∞N, {config['earthquake']['epicenter'][0]}¬∞E\")\n",
    "    print(f\"   Affected Districts: {', '.join(config['earthquake']['affected_districts'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration setup failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition\n",
    "\n",
    "Download and prepare satellite imagery from multiple sources."
   ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data acquisition with error handling\n",
    "if CUSTOM_MODULES_AVAILABLE:\n",
    "    try:\n",
    "        acquisition = EarthquakeDataAcquisition('config.json')\n",
    "        \n",
    "        # Create area of interest\n",
    "        print(\"üåç Creating Area of Interest (AOI)...\")\n",
    "        aoi = acquisition.create_aoi()\n",
    "        print(f\"   ‚úì AOI created with {config['analysis_parameters']['aoi_buffer_km']}km buffer around epicenter\")\n",
    "        \n",
    "        # Get district boundaries\n",
    "        print(\"\\nüèòÔ∏è  Retrieving district boundaries...\")\n",
    "        districts = acquisition.get_district_boundaries()\n",
    "        if districts is not None and not districts.empty:\n",
    "            print(f\"   ‚úì Found {len(districts)} affected districts: {list(districts['district'])}\")\n",
    "            DISTRICTS_AVAILABLE = True\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  District boundaries not available, creating synthetic AOI\")\n",
    "            DISTRICTS_AVAILABLE = False\n",
    "            # Create synthetic districts for demo\n",
    "            from shapely.geometry import Point, Polygon\n",
    "            import geopandas as gpd\n",
    "            \n",
    "            epicenter = config['earthquake']['epicenter']\n",
    "            buffer_deg = 0.5  # Approximately 50km\n",
    "            \n",
    "            districts_data = []\n",
    "            for i, district in enumerate(config['earthquake']['affected_districts']):\n",
    "                # Create simple polygon around epicenter\n",
    "                offset_lon = (i - 1) * 0.3\n",
    "                offset_lat = (i - 1) * 0.2\n",
    "                \n",
    "                center = Point(epicenter[0] + offset_lon, epicenter[1] + offset_lat)\n",
    "                poly = center.buffer(buffer_deg * 0.7)  # Create polygon\n",
    "                \n",
    "                districts_data.append({\n",
    "                    'district': district,\n",
    "                    'geometry': poly\n",
    "                })\n",
    "            \n",
    "            districts = gpd.GeoDataFrame(districts_data, crs='EPSG:4326')\n",
    "            print(f\"   ‚úì Created synthetic districts for demo: {list(districts['district'])}\")\n",
    "            DISTRICTS_AVAILABLE = True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data acquisition initialization failed: {e}\")\n",
    "        print(\"   Creating minimal demo data...\")\n",
    "        # Create minimal demo data\n",
    "        from shapely.geometry import Point, Polygon\n",
    "        import geopandas as gpd\n",
    "        \n",
    "        epicenter = config['earthquake']['epicenter']\n",
    "        buffer_deg = 0.5\n",
    "        \n",
    "        districts_data = []\n",
    "        for i, district in enumerate(config['earthquake']['affected_districts']):\n",
    "            offset_lon = (i - 1) * 0.3\n",
    "            offset_lat = (i - 1) * 0.2\n",
    "            center = Point(epicenter[0] + offset_lon, epicenter[1] + offset_lat)\n",
    "            poly = center.buffer(buffer_deg * 0.7)\n",
    "            districts_data.append({'district': district, 'geometry': poly})\n",
    "        \n",
    "        districts = gpd.GeoDataFrame(districts_data, crs='EPSG:4326')\n",
    "        DISTRICTS_AVAILABLE = True\n",
    "        print(\"   ‚úì Demo districts created\")\nelse:\n",
    "    print(\"üì¶ Custom modules not available - creating demo geographic data...\")\n",
    "    # Create demo data when custom modules aren't available\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    epicenter = config['earthquake']['epicenter']\n",
    "    buffer_deg = 0.5\n",
    "    \n",
    "    districts_data = []\n",
    "    for i, district in enumerate(config['earthquake']['affected_districts']):\n",
    "        offset_lon = (i - 1) * 0.3\n",
    "        offset_lat = (i - 1) * 0.2\n",
    "        center = Point(epicenter[0] + offset_lon, epicenter[1] + offset_lat)\n",
    "        poly = center.buffer(buffer_deg * 0.7)\n",
    "        districts_data.append({'district': district, 'geometry': poly})\n",
    "    \n",
    "    districts = gpd.GeoDataFrame(districts_data, crs='EPSG:4326')\n",
    "    DISTRICTS_AVAILABLE = True\n",
    "    print(\"   ‚úì Demo districts created for visualization\")"
   ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AOI and affected districts\n",
    "if DISTRICTS_AVAILABLE:\n",
    "    try:\n",
    "        print(\"üó∫Ô∏è  Creating study area visualization...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Plot districts with different colors\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "        for i, (idx, row) in enumerate(districts.iterrows()):\n",
    "            color = colors[i % len(colors)]\n",
    "            gpd.GeoDataFrame([row]).plot(ax=ax, color=color, edgecolor='black', \n",
    "                                       alpha=0.6, linewidth=2)\n",
    "        \n",
    "        # Plot epicenter\n",
    "        epicenter = config['earthquake']['epicenter']\n",
    "        ax.plot(epicenter[0], epicenter[1], 'r*', markersize=25, \n",
    "                label=f'Epicenter (M{config[\"earthquake\"][\"magnitude_ml\"]})', \n",
    "                markeredgecolor='darkred', markeredgewidth=2)\n",
    "        \n",
    "        # Add district labels\n",
    "        for idx, row in districts.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            ax.text(centroid.x, centroid.y, row['district'], \n",
    "                   ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add coordinate grid\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_title(f'Nepal Earthquake Study Area\\n{config[\"earthquake\"][\"date\"]} - M{config[\"earthquake\"][\"magnitude_ml\"]}', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Longitude (¬∞E)', fontsize=12)\n",
    "        ax.set_ylabel('Latitude (¬∞N)', fontsize=12)\n",
    "        ax.legend(fontsize=12, loc='upper right')\n",
    "        \n",
    "        # Set equal aspect ratio\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save the visualization\n",
    "        fig.savefig(data_dir / 'reports' / 'study_area_map.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        print(\"   ‚úì Study area map saved to reports/study_area_map.png\")\n",
    "        \n",
    "        # Display area statistics\n",
    "        total_area = districts.to_crs(epsg=3857).area.sum() / 1e6  # Convert to km¬≤\n",
    "        print(f\"\\nüìä Study Area Statistics:\")\n",
    "        print(f\"   Total area: {total_area:.1f} km¬≤\")\n",
    "        print(f\"   Number of districts: {len(districts)}\")\n",
    "        print(f\"   Bounding box: {districts.total_bounds}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization failed: {e}\")\n",
    "        traceback.print_exc()\nelse:\n",
    "    print(\"‚ö†Ô∏è  No district data available for visualization\")"
   ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data acquisition pipeline\n",
    "print(\"üõ∞Ô∏è  Data Acquisition Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if CUSTOM_MODULES_AVAILABLE:\n",
    "    print(\"Available data acquisition steps:\")\n",
    "    print(\"1. üîç Search for cloud-free imagery\")\n",
    "    print(\"2. üì° Create composites for pre and post earthquake periods\")\n",
    "    print(\"3. ‚òÅÔ∏è  Export imagery to Google Drive (GEE)\")\n",
    "    print(\"4. üì• Download ancillary data (OSM, elevation, population)\")\n",
    "    print(\"5. üóÇÔ∏è  Organize and catalog data\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  Prerequisites:\")\n",
    "    print(\"   - Google Earth Engine authentication\")\n",
    "    print(\"   - Sufficient Google Drive storage\")\n",
    "    print(\"   - API keys for commercial providers (optional)\")\n",
    "    \n",
    "    print(\"\\nüí° Note: Large exports may take time to process on Google Earth Engine\")\n",
    "    print(\"   Typical processing time: 15-60 minutes depending on area size\")\n",
    "    \n",
    "    # Check if user wants to run acquisition\n",
    "    print(\"\\nüîß To run full acquisition, uncomment the line below:\")\n",
    "    print(\"# acquisition.run_acquisition_pipeline()\")\n",
    "    \n",
    "    # Simulate some example data paths\n",
    "    example_data = {\n",
    "        'sentinel2_pre': data_dir / 'downloads' / 'sentinel2_pre_earthquake.tif',\n",
    "        'sentinel2_post': data_dir / 'downloads' / 'sentinel2_post_earthquake.tif',\n",
    "        'sentinel1_pre': data_dir / 'downloads' / 'sentinel1_pre_earthquake.tif',\n",
    "        'sentinel1_post': data_dir / 'downloads' / 'sentinel1_post_earthquake.tif',\n",
    "        'dem': data_dir / 'ancillary' / 'srtm_dem.tif',\n",
    "        'osm_buildings': data_dir / 'ancillary' / 'osm_buildings.geojson',\n",
    "        'population': data_dir / 'ancillary' / 'worldpop_population.tif'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìÇ Expected data outputs:\")\n",
    "    for name, path in example_data.items():\n",
    "        status = \"‚úì\" if path.exists() else \"‚è≥\"\n",
    "        print(f\"   {status} {name}: {path.name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"üì¶ Custom modules not available\")\n",
    "    print(\"   In demo mode - would normally:\")\n",
    "    print(\"   1. Authenticate with Google Earth Engine\")\n",
    "    print(\"   2. Search Sentinel-2 and Landsat collections\")\n",
    "    print(\"   3. Filter by cloud coverage and date ranges\")\n",
    "    print(\"   4. Create pre/post earthquake composites\")\n",
    "    print(\"   5. Export to Google Drive or local storage\")\n",
    "    print(\"   6. Download auxiliary datasets\")\n",
    "    \n",
    "    print(\"\\nüí° To enable full functionality:\")\n",
    "    print(\"   1. Install required custom modules\")\n",
    "    print(\"   2. Set up Google Earth Engine authentication\")\n",
    "    print(\"   3. Configure API keys in config.json\")\n",
    "\n",
    "# Create placeholder files for testing\n",
    "print(\"\\nüîß Creating placeholder data for testing...\")\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.transform import from_bounds\n",
    "    \n",
    "    # Create a small synthetic raster for testing\n",
    "    bounds = districts.total_bounds\n",
    "    width, height = 100, 100\n",
    "    transform = from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], width, height)\n",
    "    \n",
    "    # Synthetic data\n",
    "    data = np.random.randint(0, 4000, (4, height, width), dtype=np.uint16)  # 4 bands\n",
    "    \n",
    "    placeholder_files = [\n",
    "        data_dir / 'downloads' / 'sentinel2_pre_earthquake.tif',\n",
    "        data_dir / 'downloads' / 'sentinel2_post_earthquake.tif'\n",
    "    ]\n",
    "    \n",
    "    for file_path in placeholder_files:\n",
    "        with rasterio.open(file_path, 'w', driver='GTiff', height=height, width=width,\n",
    "                          count=4, dtype=data.dtype, crs='EPSG:4326', transform=transform) as dst:\n",
    "            dst.write(data)\n",
    "    \n",
    "    print(\"   ‚úì Placeholder satellite images created\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  rasterio not available - skipping placeholder creation\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not create placeholders: {e}\")"
   ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing\n",
    "\n",
    "Prepare analysis-ready data through preprocessing steps."
   ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image preprocessing workflow",
        "print(\"üîÑ Image Preprocessing Workflow\")",
        "print(\"=\" * 40)",
        "",
        "# Define paths",
        "data_dir = Path(config['data_dir'])",
        "downloads_dir = data_dir / 'downloads'",
        "processed_dir = data_dir / 'processed'",
        "processed_dir.mkdir(exist_ok=True)",
        "",
        "# Check available images",
        "print(\"üìÇ Checking for available imagery...\")",
        "if downloads_dir.exists():",
        "    available_images = list(downloads_dir.glob('*.tif'))",
        "    print(f\"   Found {len(available_images)} images:\")",
        "    for img in available_images:",
        "        try:",
        "            size_mb = img.stat().st_size / (1024 * 1024)",
        "            print(f\"     - {img.name} ({size_mb:.1f} MB)\")",
        "        except:",
        "            print(f\"     - {img.name}\")",
        "    IMAGES_AVAILABLE = len(available_images) > 0",
        "else:",
        "    print(\"   ‚ö†Ô∏è  Downloads directory not found\")",
        "    downloads_dir.mkdir(parents=True, exist_ok=True)",
        "    IMAGES_AVAILABLE = False",
        "",
        "if CUSTOM_MODULES_AVAILABLE and IMAGES_AVAILABLE:",
        "    try:",
        "        # Initialize preprocessor",
        "        print(\"\\nüîß Initializing image preprocessor...\")",
        "        preprocessor = ImagePreprocessor('config.json')",
        "        ",
        "        # Define image paths",
        "        pre_image = downloads_dir / 'sentinel2_pre_earthquake.tif'",
        "        post_image = downloads_dir / 'sentinel2_post_earthquake.tif'",
        "        aoi_file = data_dir / 'aoi' / 'affected_districts.geojson'",
        "        ",
        "        if pre_image.exists() and post_image.exists():",
        "            print(\"\\n‚öôÔ∏è  Creating Analysis-Ready Data (ARD)...\")",
        "            print(\"   Processing steps:\")",
        "            print(\"   1. üìê Co-registration\")",
        "            print(\"   2. üåü Radiometric correction\")",
        "            print(\"   3. ‚òÅÔ∏è  Cloud masking\")",
        "            print(\"   4. ‚úÇÔ∏è  Clipping to AOI\")",
        "            print(\"   5. üìè Resampling to common grid\")",
        "            ",
        "            # Save AOI for preprocessing",
        "            aoi_dir = data_dir / 'aoi'",
        "            aoi_dir.mkdir(exist_ok=True)",
        "            if DISTRICTS_AVAILABLE:",
        "                districts.to_file(aoi_dir / 'affected_districts.geojson', driver='GeoJSON')",
        "            ",
        "            ard = preprocessor.create_analysis_ready_data(",
        "                str(pre_image),",
        "                str(post_image),",
        "                str(aoi_file) if aoi_file.exists() else None",
        "            )",
        "            ",
        "            print(\"\\n‚úÖ ARD Creation Complete:\")",
        "            print(f\"   Pre-earthquake: {ard['pre']}\")",
        "            print(f\"   Post-earthquake: {ard['post']}\")",
        "            ",
        "            if 'metadata' in ard:",
        "                print(f\"\\nüìä Quality Metrics:\")",
        "                print(f\"   Pre-earthquake cloud coverage: {ard['metadata'].get('pre_cloud_percentage', 'N/A'):.2f}%\")",
        "                print(f\"   Post-earthquake cloud coverage: {ard['metadata'].get('post_cloud_percentage', 'N/A'):.2f}%\")",
        "                print(f\"   Spatial resolution: {ard['metadata'].get('resolution_m', config['analysis_parameters']['resolution_m'])}m\")",
        "                print(f\"   Coordinate system: {ard['metadata'].get('crs', 'EPSG:4326')}\")",
        "            ",
        "            ARD_AVAILABLE = True",
        "        else:",
        "            print(f\"   ‚ùå Required images not found:\")",
        "            print(f\"     Pre-earthquake: {pre_image.exists()}\")",
        "            print(f\"     Post-earthquake: {post_image.exists()}\")",
        "            ARD_AVAILABLE = False",
        "    except Exception as e:",
        "        print(f\"‚ùå Preprocessing failed: {e}\")",
        "        traceback.print_exc()",
        "        ARD_AVAILABLE = False",
        "else:",
        "    print(\"\\nüîÑ Demo Mode - Simulating preprocessing steps...\")",
        "    print(\"   1. ‚úì Image co-registration\")",
        "    print(\"   2. ‚úì Atmospheric correction\")",
        "    print(\"   3. ‚úì Cloud masking\")",
        "    print(\"   4. ‚úì Geometric correction\")",
        "    print(\"   5. ‚úì Radiometric normalization\")",
        "    ",
        "    # Create demo ARD files",
        "    if IMAGES_AVAILABLE:",
        "        try:",
        "            import shutil",
        "            ard_dir = processed_dir / 'ard'",
        "            ard_dir.mkdir(exist_ok=True)",
        "            ",
        "            # Copy and rename files for demo",
        "            pre_source = downloads_dir / 'sentinel2_pre_earthquake.tif'",
        "            post_source = downloads_dir / 'sentinel2_post_earthquake.tif'",
        "            ",
        "            if pre_source.exists() and post_source.exists():",
        "                shutil.copy2(pre_source, ard_dir / 'pre_ard.tif')",
        "                shutil.copy2(post_source, ard_dir / 'post_ard.tif')",
        "                print(\"   ‚úì Demo ARD files created\")",
        "                ARD_AVAILABLE = True",
        "            else:",
        "                ARD_AVAILABLE = False",
        "        except Exception as e:",
        "            print(f\"   ‚ö†Ô∏è  Could not create demo ARD: {e}\")",
        "            ARD_AVAILABLE = False",
        "    else:",
        "        ARD_AVAILABLE = False",
        "",
        "# Display preprocessing summary",
        "print(f\"\\nüìã Preprocessing Summary:\")",
        "print(f\"   Images available: {'‚úì' if IMAGES_AVAILABLE else '‚ùå'}\")",
        "print(f\"   ARD created: {'‚úì' if ARD_AVAILABLE else '‚ùå'}\")",
        "if ARD_AVAILABLE:",
        "    ard_dir = processed_dir / 'ard'",
        "    ard_files = list(ard_dir.glob('*.tif'))",
        "    print(f\"   ARD files: {len(ard_files)}\")",
        "    for f in ard_files:",
        "        print(f\"     - {f.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Damage Analysis\n",
        "",
        "Perform comprehensive damage assessment using multiple techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Damage analysis workflow",
        "print(\"üîç Damage Analysis Workflow\")",
        "print(\"=\" * 40)",
        "",
        "# Define input paths",
        "processed_dir = data_dir / 'processed' / 'ard'",
        "results_dir = data_dir / 'results'",
        "results_dir.mkdir(exist_ok=True)",
        "",
        "pre_ard = processed_dir / 'pre_ard.tif'",
        "post_ard = processed_dir / 'post_ard.tif'",
        "",
        "if CUSTOM_MODULES_AVAILABLE and ARD_AVAILABLE:",
        "    try:",
        "        # Initialize damage analyzer",
        "        print(\"üîß Initializing damage analyzer...\")",
        "        analyzer = DamageAnalyzer('config.json')",
        "        ",
        "        if pre_ard.exists() and post_ard.exists():",
        "            print(\"\\nüìä Running spectral analysis...\")",
        "            ",
        "            # Calculate spectral indices",
        "            print(\"   1. Computing pre-earthquake indices...\")",
        "            pre_indices, metadata = analyzer.calculate_spectral_indices(str(pre_ard))",
        "            ",
        "            print(\"   2. Computing post-earthquake indices...\")",
        "            post_indices, _ = analyzer.calculate_spectral_indices(str(post_ard))",
        "            ",
        "            print(\"   3. Calculating change detection...\")",
        "            changes = analyzer.change_detection(pre_indices, post_indices)",
        "            ",
        "            print(\"\\n‚úÖ Spectral Analysis Complete:\")",
        "            print(\"   Calculated indices:\")",
        "            for idx in pre_indices.keys():",
        "                print(f\"     - {idx.upper()}\")",
        "            ",
        "            print(\"\\n   Change indices:\")",
        "            for idx in changes.keys():",
        "                print(f\"     - {idx.upper()}\")",
        "            ",
        "            SPECTRAL_ANALYSIS_COMPLETE = True",
        "        else:",
        "            print(f\"‚ùå ARD files not found:\")",
        "            print(f\"   Pre-ARD: {pre_ard.exists()}\")",
        "            print(f\"   Post-ARD: {post_ard.exists()}\")",
        "            SPECTRAL_ANALYSIS_COMPLETE = False",
        "    except Exception as e:",
        "        print(f\"‚ùå Spectral analysis failed: {e}\")",
        "        traceback.print_exc()",
        "        SPECTRAL_ANALYSIS_COMPLETE = False",
        "else:",
        "    print(\"üì¶ Demo Mode - Simulating spectral analysis...\")",
        "    print(\"   Computing vegetation indices (NDVI, EVI, SAVI)\")",
        "    print(\"   Computing burn indices (NBR, BAI)\")",
        "    print(\"   Computing built-up indices (NDBI, BSI)\")",
        "    print(\"   Computing water indices (NDWI, MNDWI)\")",
        "    print(\"   Calculating change detection matrices\")",
        "    ",
        "    # Create synthetic spectral change data for demo",
        "    if ARD_AVAILABLE and DISTRICTS_AVAILABLE:",
        "        try:",
        "            # Create synthetic change data",
        "            bounds = districts.total_bounds",
        "            width, height = 200, 200",
        "            ",
        "            # Simulate different types of changes",
        "            np.random.seed(42)  # For reproducible results",
        "            ",
        "            changes = {",
        "                'dndvi': np.random.normal(-0.1, 0.3, (height, width)),  # Vegetation decrease",
        "                'dnbr': np.random.normal(0.15, 0.25, (height, width)),   # Burn increase",
        "                'dndbi': np.random.normal(0.05, 0.2, (height, width)),   # Built-up change",
        "                'dbsi': np.random.normal(0.08, 0.22, (height, width))    # Bare soil change",
        "            }",
        "            ",
        "            # Add some spatial structure (damage hotspots)",
        "            center_y, center_x = height // 2, width // 2",
        "            y, x = np.ogrid[:height, :width]",
        "            ",
        "            # Create damage hotspots",
        "            hotspot1 = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (50**2))",
        "            hotspot2 = np.exp(-((x - center_x + 60)**2 + (y - center_y - 40)**2) / (30**2))",
        "            hotspot3 = np.exp(-((x - center_x - 70)**2 + (y - center_y + 50)**2) / (40**2))",
        "            ",
        "            damage_pattern = hotspot1 + 0.7 * hotspot2 + 0.5 * hotspot3",
        "            ",
        "            # Apply damage pattern to indices",
        "            changes['dndvi'] -= damage_pattern * 0.5  # More vegetation loss in damage areas",
        "            changes['dnbr'] += damage_pattern * 0.3   # More disturbance",
        "            changes['dndbi'] += damage_pattern * 0.4  # More built-up change",
        "            ",
        "            SPECTRAL_ANALYSIS_COMPLETE = True",
        "            print(\"   ‚úì Synthetic spectral change data created\")",
        "        except Exception as e:",
        "            print(f\"   ‚ö†Ô∏è  Could not create synthetic data: {e}\")",
        "            SPECTRAL_ANALYSIS_COMPLETE = False",
        "    else:",
        "        SPECTRAL_ANALYSIS_COMPLETE = False",
        "",
        "print(f\"\\nüìã Analysis Status: {'‚úì' if SPECTRAL_ANALYSIS_COMPLETE else '‚ùå'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize spectral changes\n",
        "if SPECTRAL_ANALYSIS_COMPLETE and 'changes' in locals():\n",
        "    try:\n",
        "        print(\"üìä Creating spectral change visualizations...\")\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        axes = axes.ravel()\n",
        "        \n",
        "        # Define indices to plot with descriptions\n",
        "        indices_info = {\n",
        "            'dndvi': {\n",
        "                'title': 'ŒîNDVI (Vegetation Change)',\n",
        "                'description': 'Negative = vegetation loss/damage',\n",
        "                'cmap': 'RdYlGn'\n",
        "            },\n",
        "            'dnbr': {\n",
        "                'title': 'ŒîNBR (Burn/Disturbance)',\n",
        "                'description': 'Positive = increased disturbance',\n",
        "                'cmap': 'RdYlBu_r'\n",
        "            },\n",
        "            'dndbi': {\n",
        "                'title': 'ŒîNDBI (Built-up Change)',\n",
        "                'description': 'Changes in built environment',\n",
        "                'cmap': 'RdBu_r'\n",
        "            },\n",
        "            'dbsi': {\n",
        "                'title': 'ŒîBSI (Bare Soil Change)',\n",
        "                'description': 'Changes in exposed soil/debris',\n",
        "                'cmap': 'RdGy_r'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        for i, (idx, info) in enumerate(indices_info.items()):\n",
        "            if idx in changes and i < 4:\n",
        "                # Calculate statistics\n",
        "                data = changes[idx]\n",
        "                vmin, vmax = np.percentile(data, [5, 95])\n",
        "                vmax = max(abs(vmin), abs(vmax))  # Symmetric scale\n",
        "                vmin = -vmax\n",
        "                \n",
        "                im = axes[i].imshow(data, cmap=info['cmap'], vmin=vmin, vmax=vmax)\n",
        "                axes[i].set_title(f\"{info['title']}\\n{info['description']}\", \n",
        "                                fontsize=11, pad=10)\n",
        "                axes[i].axis('off')\n",
        "                \n",
        "                # Add colorbar\n",
        "                cbar = plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
        "                cbar.set_label('Change Index', fontsize=9)\n",
        "                \n",
        "                # Add statistics text\n",
        "                mean_change = np.mean(data)\n",
        "                std_change = np.std(data)\n",
        "                axes[i].text(0.02, 0.98, f'Œº={mean_change:.3f}\\nœÉ={std_change:.3f}',\n",
        "                           transform=axes[i].transAxes, fontsize=8,\n",
        "                           verticalalignment='top',\n",
        "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
        "        \n",
        "        plt.suptitle('Spectral Index Changes (Post - Pre Earthquake)\\n' +\n",
        "                    f'{config[\"earthquake\"][\"date\"]} - {config[\"earthquake\"][\"location\"]}', \n",
        "                    fontsize=14, fontweight='bold', y=0.98)\n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(top=0.92)\n",
        "        plt.show()\n",
        "        \n",
        "        # Save the visualization\n",
        "        fig.savefig(results_dir / 'spectral_changes.png', \n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        print(\"   ‚úì Spectral change plots saved\")\n",
        "        \n",
        "        # Calculate and display change statistics\n",
        "        print(\"\\nüìà Change Statistics:\")\n",
        "        for idx, data in changes.items():\n",
        "            if idx in indices_info:\n",
        "                high_change_pixels = np.sum(np.abs(data) > 0.2)\n",
        "                total_pixels = data.size\n",
        "                pct_changed = (high_change_pixels / total_pixels) * 100\n",
        "                print(f\"   {idx.upper()}: {pct_changed:.1f}% of area shows significant change\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Visualization failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No spectral change data available for visualization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive damage assessment\n",
        "print(\"üèóÔ∏è  Comprehensive Damage Assessment\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "if CUSTOM_MODULES_AVAILABLE and ARD_AVAILABLE:\n",
        "    try:\n",
        "        print(\"üîÑ Running comprehensive assessment...\")\n",
        "        print(\"   Analysis components:\")\n",
        "        print(\"   1. üåø Spectral change analysis\")\n",
        "        print(\"   2. üßÆ Texture analysis\")\n",
        "        print(\"   3. ü§ñ Machine learning classification\")\n",
        "        print(\"   4. üè† Building damage assessment\")\n",
        "        print(\"   5. üèîÔ∏è  Landslide detection\")\n",
        "        print(\"   6. üìä Statistical analysis\")\n",
        "        \n",
        "        # Define additional input files\n",
        "        pre_sar = processed_dir / 'sentinel1_pre_earthquake.tif'\n",
        "        post_sar = processed_dir / 'sentinel1_post_earthquake.tif'\n",
        "        buildings = data_dir / 'ancillary' / 'osm_buildings.geojson'\n",
        "        slope = data_dir / 'ancillary' / 'slope.tif'\n",
        "        \n",
        "        # Run comprehensive assessment\n",
        "        results = analyzer.comprehensive_damage_assessment(\n",
        "            str(pre_ard),\n",
        "            str(post_ard),\n",
        "            str(pre_sar) if pre_sar.exists() else None,\n",
        "            str(post_sar) if post_sar.exists() else None,\n",
        "            str(buildings) if buildings.exists() else None,\n",
        "            str(slope) if slope.exists() else None\n",
        "        )\n",
        "        \n",
        "        print(\"\\n‚úÖ Assessment Complete!\")\n",
        "        print(f\"   Results saved to: {analyzer.results_dir}\")\n",
        "        \n",
        "        # Display statistics\n",
        "        if 'statistics' in results:\n",
        "            print(\"\\nüìä Damage Statistics:\")\n",
        "            stats = results['statistics']\n",
        "            if 'damage_distribution' in stats:\n",
        "                for level, data in stats['damage_distribution'].items():\n",
        "                    area_km2 = data.get('area_km2', 0)\n",
        "                    percentage = data.get('percentage', 0)\n",
        "                    print(f\"     {level}: {area_km2:.2f} km¬≤ ({percentage:.1f}%)\")\n",
        "        \n",
        "        COMPREHENSIVE_ASSESSMENT_COMPLETE = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Comprehensive assessment failed: {e}\")\n",
        "        traceback.print_exc()\n",
        "        COMPREHENSIVE_ASSESSMENT_COMPLETE = False\n",
        "\n",
        "else:\n",
        "    print(\"üì¶ Demo Mode - Simulating comprehensive assessment...\")\n",
        "    \n",
        "    # Create synthetic damage classification\n",
        "    if SPECTRAL_ANALYSIS_COMPLETE and 'changes' in locals():\n",
        "        try:\n",
        "            print(\"   Creating synthetic damage classification...\")\n",
        "            \n",
        "            # Combine multiple indices for damage classification\n",
        "            damage_score = np.zeros_like(changes['dndvi'])\n",
        "            \n",
        "            # Weight different indices\n",
        "            damage_score += -changes['dndvi'] * 0.4  # Vegetation loss (negative NDVI change)\n",
        "            damage_score += changes['dnbr'] * 0.3    # Disturbance\n",
        "            damage_score += np.abs(changes['dndbi']) * 0.2  # Built-up changes\n",
        "            damage_score += changes['dbsi'] * 0.1    # Bare soil increase\n",
        "            \n",
        "            # Normalize score\n",
        "            damage_score = (damage_score - np.min(damage_score)) / (np.max(damage_score) - np.min(damage_score))\n",
        "            \n",
        "            # Classify damage levels\n",
        "            damage_classification = np.zeros_like(damage_score, dtype=np.uint8)\n",
        "            damage_classification[damage_score < 0.2] = 0  # No damage\n",
        "            damage_classification[(damage_score >= 0.2) & (damage_score < 0.4)] = 1  # Low damage\n",
        "            damage_classification[(damage_score >= 0.4) & (damage_score < 0.6)] = 2  # Moderate damage\n",
        "            damage_classification[(damage_score >= 0.6) & (damage_score < 0.8)] = 3  # High damage\n",
        "            damage_classification[damage_score >= 0.8] = 4  # Severe damage\n",
        "            \n",
        "            # Calculate statistics\n",
        "            total_pixels = damage_classification.size\n",
        "            damage_levels = ['No Damage', 'Low', 'Moderate', 'High', 'Severe']\n",
        "            \n",
        "            print(\"\\nüìä Synthetic Damage Assessment Results:\")\n",
        "            for i, level in enumerate(damage_levels):\n",
        "                count = np.sum(damage_classification == i)\n",
        "                percentage = (count / total_pixels) * 100\n",
        "                area_km2 = percentage * 0.5  # Assume 50 km¬≤ total area for demo\n",
        "                print(f\"     {level}: {area_km2:.2f} km¬≤ ({percentage:.1f}%)\")\n",
        "            \n",
        "            # Save synthetic results\n",
        "            synthetic_results = {\n",
        "                'damage_classification': damage_classification,\n",
        "                'damage_score': damage_score,\n",
        "                'statistics': {\n",
        "                    'total_area_km2': 50.0,\n",
        "                    'damage_distribution': {}\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            for i, level in enumerate(damage_levels):\n",
        "                count = np.sum(damage_classification == i)\n",
        "                percentage = (count / total_pixels) * 100\n",
        "                area_km2 = percentage * 0.5\n",
        "                synthetic_results['statistics']['damage_distribution'][level] = {\n",
        "                    'area_km2': area_km2,\n",
        "                    'percentage': percentage\n",
        "                }\n",
        "            \n",
        "            # Save to results directory\n",
        "            np.save(results_dir / 'damage_classification.npy', damage_classification)\n",
        "            np.save(results_dir / 'damage_score.npy', damage_score)\n",
        "            \n",
        "            with open(results_dir / 'damage_assessment_report.json', 'w') as f:\n",
        "                json.dump(synthetic_results['statistics'], f, indent=4)\n",
        "            \n",
        "            results = synthetic_results\n",
        "            COMPREHENSIVE_ASSESSMENT_COMPLETE = True\n",
        "            print(\"   ‚úì Synthetic damage assessment complete\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not create synthetic assessment: {e}\")\n",
        "            COMPREHENSIVE_ASSESSMENT_COMPLETE = False\n",
        "    else:\n",
        "        COMPREHENSIVE_ASSESSMENT_COMPLETE = False\n",
        "        print(\"   ‚ö†Ô∏è  No spectral data available for synthetic assessment\")\n",
        "\n",
        "print(f\"\\nüìã Assessment Status: {'‚úì' if COMPREHENSIVE_ASSESSMENT_COMPLETE else '‚ùå'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "capexdx",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
  }